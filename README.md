# local-llama3-sentiment-analysis

Download ollama 

download llama3 model by 

`ollama pull llama3`


Set up the local ollama server first with 

`ollama serve`

then run the code.

if port 11434 is occupied, just do 

`sudo systemctl stop ollama`

then 

`sudo systemctl start ollama`

then run 

`ollama serve`
